@incollection{Fayek2024,
  author = {Haytham M. Fayek and Hong Ren Wu},
  title = {Architectural Approaches to Continual Learning},
  booktitle = {Towards Human Brain Inspired Lifelong Learning},
  chapter = {2},
  pages = {9--23},
  doi = {10.1142/9789811286711_0002},
  URL = {https://www.worldscientific.com/doi/abs/10.1142/9789811286711_0002},
  eprint = {https://www.worldscientific.com/doi/pdf/10.1142/9789811286711_0002},
  abstract = {Continual learning is the ability of a learning system to solve new tasks by utilizing previously acquired knowledge from learning and performing prior tasks without having significant adverse effects on the acquired prior knowledge. Numerous approaches were developed to achieve this ability while avoiding the well-known problem of catastrophic forgetting in neural networks. This chapter reviews a number of architectural approaches to continual learning in neural networks which tackle the problem by modifying the architecture of the neural network, for example, by adding new adaptive parameters to the model for each new task. The architectural paradigm for continual learning in neural networks can potentially completely eliminate the problem of catastrophic forgetting and maintain competitive performance, often at the expense of increased computational complexity.},
}
