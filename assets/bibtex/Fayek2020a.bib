@article{Fayek2020a,
  abstract = {Continual learning is the ability of a learning system to solve new tasks by utilizing previously acquired knowledge from learning and performing prior tasks without having significant adverse effects on the acquired prior knowledge. Continual learning is key to advancing machine learning and artificial intelligence. Progressive learning is a deep learning framework for continual learning that comprises three procedures: curriculum, progression, and pruning. The curriculum procedure is used to actively select a task to learn from a set of candidate tasks. The progression procedure is used to grow the capacity of the model by adding new parameters that leverage parameters learned in prior tasks, while learning from data available for the new task at hand, without being susceptible to catastrophic forgetting. The pruning procedure is used to counteract the growth in the number of parameters as further tasks are learned, as well as to mitigate negative forward transfer, in which prior knowledge unrelated to the task at hand may interfere and worsen performance. Progressive learning is evaluated on a number of supervised classification tasks in the image recognition and speech recognition domains to demonstrate its advantages compared with baseline methods. It is shown that, when tasks are related, progressive learning leads to faster learning that converges to better generalization performance using a smaller number of dedicated parameters.},
  author = {Haytham M. Fayek and Lawrence Cavedon and Hong Ren Wu},
  doi = {https://doi.org/10.1016/j.neunet.2020.05.011},
  issn = {0893-6080},
  journal = {Neural Networks},
  keywords = {Continual learning, Computer vision, Deep learning, Machine learning, Neural networks, Speech recognition},
  pages = {345--357},
  title = {Progressive learning: A deep learning framework for continual learning},
  url = {http://www.sciencedirect.com/science/article/pii/S0893608020301817},
  volume = {128},
  year = {2020},
}
